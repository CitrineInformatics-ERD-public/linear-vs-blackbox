{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "698cbe27",
   "metadata": {},
   "source": [
    "\n",
    "> **Distribution A**\n",
    "> Approved for Public Release, Distribution Unlimited\n",
    "\n",
    "# Workflow for reproducing *Interpretable models for extrapolation in scientific machine learning*\n",
    "\n",
    "General procedure:\n",
    "* Import the config file which contains dataset information\n",
    "* Import all datasets from CSV files as Pandas DataFrames\n",
    "* Featurize the Matminer datasets using Magpie features\n",
    "* Clean all datasets\n",
    "* Export all the clean featurized datasets to a single JSON file\n",
    "* Partition the datasets into train-test-validation sets using LOCO CV (for extrapolation) and random CV (for interpolation)\n",
    "* Perform combinatorial feature engineering on the datasets\n",
    "    * for each dataset:\n",
    "        * for each CV strategy:\n",
    "            * for each train-test split:\n",
    "                * perform feature engineering\n",
    "                * rank engineered features\n",
    "                * save top 10 engineered features\n",
    "* Train and test ML models and linear regressions using original features and engineered features\n",
    "* Analyze results of the model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ee1332",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "from matplotlib.pyplot import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "import utils\n",
    "\n",
    "FONTSIZE = 10\n",
    "LINEWIDTH = 0.5\n",
    "TICKWIDTH = 0.5\n",
    "plt.rcParams.update(\n",
    "    {\n",
    "        \"xtick.labelsize\": FONTSIZE,\n",
    "        \"ytick.labelsize\": FONTSIZE,\n",
    "        \"axes.linewidth\": LINEWIDTH,\n",
    "        \"xtick.minor.width\": TICKWIDTH,\n",
    "        \"xtick.major.width\": TICKWIDTH,\n",
    "        \"ytick.minor.width\": TICKWIDTH,\n",
    "        \"ytick.major.width\": TICKWIDTH,\n",
    "        \"font.family\": \"Arial\",\n",
    "        \"figure.facecolor\": \"w\",\n",
    "        \"figure.dpi\": 600,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c019763e",
   "metadata": {},
   "source": [
    "# Create and view datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41850d66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "utils.create_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c604da4b",
   "metadata": {},
   "source": [
    "# Get cross-validation splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cde91d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import datasets and create CV splits\n",
    "ds = utils.read_jsonzip(os.path.join(\"data\", \"datasets.json.gz\"))\n",
    "splits = utils.get_cv_splits(ds, klims=(3, 10))\n",
    "\n",
    "# use this to view the size of each split\n",
    "#utils.view_split_sizes(splits)\n",
    "\n",
    "utils.write_jsonzip(splits, os.path.join(\"data\", \"splits.json.gz\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a7be43ae",
   "metadata": {},
   "source": [
    "# Visualize LOCO splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22041922",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "utils.view_all_splits(\"LOCO\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e6ba13f4",
   "metadata": {},
   "source": [
    "# Visualize Random splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9129598f",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.view_all_splits(\"Random\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d58185c1-3633-47da-aee0-284600f72e9b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Engineer new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cf94e0-e00d-4f8e-9ba0-61375145f248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data\n",
    "ds = utils.read_jsonzip(os.path.join(\"data\", \"datasets.json.gz\"))\n",
    "splits = utils.read_jsonzip(os.path.join(\"data\", \"splits.json.gz\"))\n",
    "split_types = [\"LOCO\", \"Random\"]\n",
    "\n",
    "# create new engineered features\n",
    "N_COMBOS = 500000  # number of combos to calculate\n",
    "N_LIMS_PER_COMBO = (2, 5)  # min and max number of columns per combo\n",
    "N_SAVE = 10  # number of top discovered features to save for each split\n",
    "\n",
    "fs = {dsn: {} for dsn in ds}\n",
    "starttime = time.time()\n",
    "\n",
    "# loop over each dataset\n",
    "for di, dsn in enumerate(list(ds)):\n",
    "\n",
    "    # get dataset and r^2 between the best original input feature and the target\n",
    "    print(f\"{di+1}: {dsn}\")\n",
    "    df = pd.DataFrame(json.loads(ds[dsn][\"df\"]))\n",
    "    t = ds[dsn][\"target\"]\n",
    "    target_vals = df[t].values\n",
    "    prev_rtot = (\n",
    "        df.corrwith(df[t], method=utils.get_r2).sort_values(ascending=False).iloc[1]\n",
    "    )\n",
    "\n",
    "    # create additional input columns based on existing columns\n",
    "    dfa = utils.add_additional_cols(df, ignore=[t])\n",
    "    n_inputs_for_mixing = dfa.shape[1] - 1\n",
    "    fs[dsn][\"n_inputs_for_mixing\"] = n_inputs_for_mixing\n",
    "    print(f\"{dsn}: {n_inputs_for_mixing} inputs\")\n",
    "\n",
    "    # create random combinations of input columns\n",
    "    col_combos = utils.random_combos(\n",
    "        inputs=list(set(utils.get_num_cols(dfa, ignore=[t]))),\n",
    "        n_combos=N_COMBOS,\n",
    "        lims=N_LIMS_PER_COMBO,\n",
    "    )\n",
    "\n",
    "    # loop over each split to find the best new features for that split\n",
    "    for split_type in split_types:\n",
    "        fs[dsn][split_type] = []\n",
    "\n",
    "        print(f\"  {split_type}\")\n",
    "        ss = splits[dsn][split_type]\n",
    "\n",
    "        for si, s in enumerate(ss):\n",
    "\n",
    "            # get indices of this validation set\n",
    "            valid_idx = [df.index.tolist().index(i) for i in s[\"validation\"]]\n",
    "            target_valid = target_vals[valid_idx]\n",
    "\n",
    "            correlation_dicts = []\n",
    "\n",
    "            # loop over each new column combo and get validation set at this split\n",
    "            for cc in col_combos:\n",
    "\n",
    "                # get values of then new feature and target at this validation set\n",
    "                data = utils.vals_from_combos(dfa, cc)\n",
    "                if data is None:\n",
    "                    continue\n",
    "\n",
    "                data_valid = data[valid_idx]\n",
    "                if not utils.unique_enough(data_valid):\n",
    "                    continue\n",
    "\n",
    "                # get r^2 correlations between the target and each engineered feature\n",
    "                correlation = utils.get_r2(data_valid, target_valid)\n",
    "                if np.isfinite(correlation):\n",
    "                    correlation_dicts.append({\"feat\": cc, \"rtot\": correlation})\n",
    "\n",
    "            # sort by the most highly correlated features\n",
    "            correlation_dicts = sorted(\n",
    "                correlation_dicts, key=lambda d: d[\"rtot\"], reverse=True\n",
    "            )[:N_SAVE]\n",
    "\n",
    "            # save results to dataframe\n",
    "            for cd in correlation_dicts:\n",
    "                cd[\"val\"] = utils.vals_from_combos(dfa, cd[\"feat\"])\n",
    "                cd[\"feat\"] = utils.refactor_feat(cd[\"feat\"])\n",
    "            corrs = pd.DataFrame(correlation_dicts)\n",
    "            fs[dsn][split_type].append(corrs.to_json(default_handler=str))\n",
    "\n",
    "            # determine whether new features beat the old features\n",
    "            new_rtot = corrs[\"rtot\"].iloc[0]\n",
    "            if new_rtot < prev_rtot:\n",
    "                print(f\"\\tsplit {si}: no improvement\")\n",
    "            else:\n",
    "                print(\n",
    "                    f\"\\tsplit {si}: r^2 {round(prev_rtot, 2)} -> {round(new_rtot, 2)}\"\n",
    "                )\n",
    "\n",
    "print(f\"\\nFeature discovery runtime: {round((time.time() - starttime)/60, 2)} min\")\n",
    "# save features to file\n",
    "utils.write_jsonzip(fs, os.path.join(\"data\", \"features.json.gz\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d0b052",
   "metadata": {},
   "source": [
    "# Plot target vs best feature for all datasets and LOCO splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402d8e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_target_vs_best_feature()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ba25b8",
   "metadata": {},
   "source": [
    "# Plot feature correlations with targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80503a42-91b0-4206-a3d5-ccdbef5b2eef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_feature_correlations_w_targets():\n",
    "    \"\"\"Plot Pearson and Spearman correlations between\n",
    "    all input features and the target feature\"\"\"\n",
    "    fig, ax = plt.subplots(nrows=3, ncols=3)\n",
    "    ax = np.ravel(ax)\n",
    "    ii = 0\n",
    "    CV_COLORS = {\n",
    "        \"Original\": \"black\",\n",
    "        \"LOCO\": \"red\",\n",
    "        \"Random\": \"blue\",\n",
    "    }\n",
    "    ALPHA = 0.2\n",
    "\n",
    "    ds = utils.read_jsonzip(os.path.join(\"data\", \"datasets.json.gz\"))\n",
    "    fs = utils.read_jsonzip(os.path.join(\"data\", \"features.json.gz\"))\n",
    "\n",
    "    for dsn in ds:\n",
    "        print(f\"\\n{dsn}\")\n",
    "        t = ds[dsn][\"target\"]\n",
    "        df = pd.DataFrame(json.loads(ds[dsn][\"df\"]))\n",
    "\n",
    "        for split_type in [\"Original\", \"LOCO\", \"Random\"]:\n",
    "            r2s, sr2s = [], []\n",
    "\n",
    "            if split_type == \"Original\":\n",
    "                for c in utils.get_num_cols(df, ignore=[t]):\n",
    "                    x, y = df[c].values, df[t].values\n",
    "                    r2s.append(np.square(stats.pearsonr(x, y)[0]))\n",
    "                    sr2s.append(np.square(stats.spearmanr(x, y)[0]))\n",
    "                best_idx = np.argmax(r2s)\n",
    "                highlight = (sr2s[best_idx], r2s[best_idx])\n",
    "\n",
    "            else:\n",
    "                # get pearson and spearman r^2 values across all new features\n",
    "                new_feat_df = pd.concat(\n",
    "                    [pd.DataFrame(json.loads(i)) for i in fs[dsn][split_type]]\n",
    "                )\n",
    "                for i, row in new_feat_df.iterrows():\n",
    "                    x, y = row[\"val\"], df[t].values\n",
    "                    r2s.append(np.square(stats.pearsonr(x, y)[0]))\n",
    "                    sr2s.append(np.square(stats.spearmanr(x, y)[0]))\n",
    "                best_idx = np.argmax(r2s)\n",
    "                highlight = (np.median(sr2s), np.median(r2s))\n",
    "\n",
    "            # plot highlighted location\n",
    "            ax[ii].scatter(\n",
    "                highlight[0],\n",
    "                highlight[1],\n",
    "                s=150,\n",
    "                c=CV_COLORS[split_type],\n",
    "                marker=\"+\",\n",
    "                lw=1,\n",
    "                zorder=99,\n",
    "            )\n",
    "\n",
    "            # plot all features\n",
    "            ax[ii].scatter(\n",
    "                sr2s,\n",
    "                r2s,\n",
    "                lw=0.25,\n",
    "                s=6,\n",
    "                edgecolors=\"w\",\n",
    "                alpha=ALPHA,\n",
    "                c=CV_COLORS[split_type],\n",
    "            )\n",
    "            print(\n",
    "                f\"{split_type} best r2: {round(r2s[best_idx], 3)}, best sr2: {round(sr2s[best_idx], 3)}\"\n",
    "            )\n",
    "\n",
    "            # add confidence ellipse\n",
    "            utils.confidence_ellipse(\n",
    "                sr2s,\n",
    "                r2s,\n",
    "                ax[ii],\n",
    "                n_std=1,\n",
    "                facecolor=\"none\",\n",
    "                lw=0.5,\n",
    "                edgecolor=CV_COLORS[split_type],\n",
    "                linestyle=\"solid\",\n",
    "            )\n",
    "\n",
    "        ax[ii].tick_params(labelsize=FONTSIZE)\n",
    "        # ax[ii].set_xticks([0, 0.5, 1], [0, 0.5, 1])\n",
    "        # ax[ii].set_yticks([0, 0.5, 1], [0, 0.5, 1])\n",
    "        ax[ii].set_xlim([0, 1])\n",
    "        ax[ii].set_ylim([0, 1])\n",
    "\n",
    "        if ii in [0, 3, 6]:\n",
    "            ax[ii].set_ylabel(\"r$^2$\", fontsize=FONTSIZE)\n",
    "\n",
    "        else:\n",
    "            ax[ii].set_ylabel(\"\")\n",
    "            ax[ii].set_yticklabels([])\n",
    "        if ii in [6, 7, 8]:\n",
    "            ax[ii].set_xlabel(\"ρ$^2$\", fontsize=FONTSIZE)\n",
    "        else:\n",
    "            ax[ii].set_xlabel(\"\")\n",
    "            ax[ii].set_xticklabels([])\n",
    "\n",
    "        ax[ii].text(\n",
    "            0.03,\n",
    "            0.96,\n",
    "            t,\n",
    "            fontsize=FONTSIZE,\n",
    "            ha=\"left\",\n",
    "            va=\"top\",\n",
    "            transform=ax[ii].transAxes,\n",
    "        )\n",
    "\n",
    "        ax[ii].axvline(\n",
    "            x=0.5, color=\"gray\", lw=1, linestyle=\"dotted\", alpha=0.3, zorder=0\n",
    "        )\n",
    "        ax[ii].axhline(\n",
    "            y=0.5, color=\"gray\", lw=1, linestyle=\"dotted\", alpha=0.3, zorder=0\n",
    "        )\n",
    "        ii += 1\n",
    "\n",
    "    ax[1].legend(\n",
    "        ncol=3,\n",
    "        bbox_to_anchor=(0.5, 1.05),\n",
    "        loc=\"lower center\",\n",
    "        # fontsize=16,\n",
    "        handles=[\n",
    "            mpatches.Patch(\n",
    "                color=CV_COLORS[k],\n",
    "                alpha=1,\n",
    "                label=k,\n",
    "            )\n",
    "            for k in CV_COLORS\n",
    "        ],\n",
    "    )\n",
    "    plt.subplots_adjust(wspace=-1.2, hspace=0.4)\n",
    "    plt.tight_layout()\n",
    "    plt.gcf().savefig(os.path.join(utils.FIG_BASEPATH, \"FeatureCorrelations.png\"))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_feature_correlations_w_targets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d0060a",
   "metadata": {},
   "source": [
    "# Get frequency of variables in top-performing features across all CV splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337e1cf2-83f3-4eeb-a715-8908eba8c546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_var(x: str) -> str:\n",
    "    \"\"\"get base variable from engineered variable\"\"\"\n",
    "    return x.split(\" \")[-1].split(\"**\")[0].replace(\"ln \", \"\")\n",
    "\n",
    "\n",
    "# import the data\n",
    "ds = utils.read_jsonzip(os.path.join(\"data\", \"datasets.json.gz\"))\n",
    "splits = utils.read_jsonzip(os.path.join(\"data\", \"splits.json.gz\"))\n",
    "fs = utils.read_jsonzip(os.path.join(\"data\", \"features.json.gz\"))\n",
    "split_types = [\"LOCO\", \"Random\"]\n",
    "CV_COLORS = {\"LOCO\": \"tomato\", \"Random\": \"dodgerblue\"}\n",
    "\n",
    "num_input_col_dict = {dsn: v[\"n_inputs_for_mixing\"] for dsn, v in fs.items()}\n",
    "\n",
    "# create figure for target value histograms\n",
    "fig, ax = plt.subplots(nrows=3, ncols=3)\n",
    "ax = np.ravel(ax)\n",
    "ii = 0\n",
    "\n",
    "n = 5  # number of features to plot in each panel\n",
    "colors = cm.Set2(np.linspace(0, 1, n))[::-1]\n",
    "\n",
    "# loop over each dataset\n",
    "for di, dsn in enumerate(list(ds)):\n",
    "    df = pd.DataFrame(json.loads(ds[dsn][\"df\"]))\n",
    "    t = ds[dsn][\"target\"]\n",
    "\n",
    "    # total number of input features in this dataset\n",
    "    n_inputs = len(utils.get_num_cols(df)) - 1\n",
    "\n",
    "    for st in split_types:\n",
    "\n",
    "        # get all the feature combos saved for this dataset and split\n",
    "        feat_list = []\n",
    "\n",
    "        for si in range(len(fs[dsn][st])):\n",
    "            featdf = pd.DataFrame(json.loads(fs[dsn][st][si]))\n",
    "\n",
    "            feat_list += [ff for fl in list(featdf[\"feat\"]) for ff in fl]\n",
    "\n",
    "        # get ranking of frequencies at which each variable shows up in engineered features\n",
    "        var_freqs = pd.DataFrame(\n",
    "            data=Counter(feat_list).most_common(), columns=[\"feat\", \"freq\"]\n",
    "        )\n",
    "\n",
    "        # expectation value is the total number of splits a feature should show up in on average.\n",
    "        # use this number as the denominator to calculate the fraction of splits.\n",
    "        # n_splits per strategy * n_top_features per split *\n",
    "        # 3.5 (mean # of cols per engineered feat) /\n",
    "        # number of possible input cols\n",
    "        expectation_value = 10 * 10 * 3.5 / num_input_col_dict[dsn]\n",
    "        x, y = list(var_freqs[\"feat\"])[:n][::-1], var_freqs[\"freq\"][:n][::-1].values\n",
    "\n",
    "        # print(dsn, st, y)\n",
    "        # print(expectation_value)\n",
    "        if st == \"LOCO\":\n",
    "            loco_vals = y / expectation_value\n",
    "            bars_loco = ax[ii].barh(\n",
    "                np.arange(len(x)),\n",
    "                y / expectation_value,\n",
    "                height=0.5,\n",
    "                lw=0,\n",
    "                alpha=0.75,\n",
    "                color=CV_COLORS[\"LOCO\"],\n",
    "            )\n",
    "        else:\n",
    "            bars_random = ax[ii].barh(\n",
    "                np.arange(len(x)),\n",
    "                y / expectation_value,\n",
    "                height=0.5,\n",
    "                lw=0,\n",
    "                alpha=0.75,\n",
    "                left=loco_vals,\n",
    "                color=CV_COLORS[\"Random\"],\n",
    "            )\n",
    "\n",
    "    ax[ii].set_yticks(np.arange(len(x)))\n",
    "    ax[ii].set_yticklabels(utils.format_labels(x))\n",
    "    ax[ii].set_ylim([-0.4, len(x) - 0.6])\n",
    "    ax[ii].set_xlim([0, 100])\n",
    "    ax[ii].set_xticks([0, 25, 50, 75, 100])\n",
    "\n",
    "    for x0 in [25, 50, 75]:\n",
    "        ax[ii].axvline(x=x0, linestyle=\"solid\", lw=0.1, c=\"gray\")\n",
    "\n",
    "\n",
    "    base_vars = [get_base_var(x0) for x0 in x]\n",
    "    for bi in range(len(bars_loco)):\n",
    "        if base_vars.count(get_base_var(x[bi])) > 1:\n",
    "\n",
    "            ax[ii].get_yticklabels()[bi].set_color(\"purple\")\n",
    "\n",
    "    if ii > 5:\n",
    "        ax[ii].set_xticklabels([0, 25, 50, 75, 100])\n",
    "    else:\n",
    "        ax[ii].set_xticklabels([])\n",
    "\n",
    "    ax[ii].text(\n",
    "        0.5,\n",
    "        1.03,\n",
    "        t,\n",
    "        horizontalalignment=\"center\",\n",
    "        fontsize=FONTSIZE,\n",
    "        transform=ax[ii].transAxes,\n",
    "    )\n",
    "\n",
    "    ii += 1\n",
    "\n",
    "\n",
    "ax[1].legend(\n",
    "    ncol=2,\n",
    "    bbox_to_anchor=(0.5, 1.15),\n",
    "    loc=\"lower center\",\n",
    "    # fontsize=16,\n",
    "    handles=[\n",
    "        mpatches.Patch(\n",
    "            color=CV_COLORS[k],\n",
    "            alpha=1,\n",
    "            label=k,\n",
    "        )\n",
    "        for k in CV_COLORS\n",
    "    ],\n",
    ")\n",
    "\n",
    "ax[7].set_xlabel(\"Frequency / expectation value\", fontsize=FONTSIZE)\n",
    "fig.set_size_inches(9, 5)\n",
    "plt.subplots_adjust(wspace=1.7, hspace=0.5)\n",
    "# plt.xlabel(\"Frequency\")\n",
    "plt.tight_layout()\n",
    "plt.gcf().savefig(\n",
    "    os.path.join(utils.FIG_BASEPATH, \"FeatureVariableProminence.png\"),\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44d4869",
   "metadata": {},
   "source": [
    "# Display best features based on their correlation to full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eb4d96-feec-4b7b-8130-9d60dce1df5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read datasets\n",
    "ds = utils.read_jsonzip(os.path.join(\"data\", \"datasets.json.gz\"))\n",
    "fs = utils.read_jsonzip(os.path.join(\"data\", \"features.json.gz\"))\n",
    "split_types = [\"LOCO\", \"Random\"]\n",
    "\n",
    "for dsn in fs:\n",
    "    t = ds[dsn][\"target\"]\n",
    "    print(t)\n",
    "    \n",
    "    # get full dataset with augmented features\n",
    "    df = pd.DataFrame(json.loads(ds[dsn][\"df\"]))\n",
    "    dfa = utils.add_additional_cols(df, ignore=[t])\n",
    "    target_vals = df[t]\n",
    "\n",
    "    r2max = 0\n",
    "    best_feat = None\n",
    "    \n",
    "    # iterate over all the engineered features in this dataset\n",
    "    # iterate over split type\n",
    "    for st in split_types:\n",
    "        # iterate over each split\n",
    "        for si in range(len(fs[dsn][st])):\n",
    "            # iterate over the 10 engineered feastures saved for this split\n",
    "            feats = [v for _, v in json.loads(fs[dsn][st][si])[\"feat\"].items()]\n",
    "            for feat in feats:\n",
    "                \n",
    "                # get correlation of this feature with the full dataset\n",
    "                feature_vals = utils.vals_from_combos(dfa, feat)\n",
    "                r2 = utils.get_r2(target_vals, feature_vals)\n",
    "                \n",
    "                if r2 > r2max:\n",
    "                    r2max = r2\n",
    "                    best_feat = feat\n",
    "                    \n",
    "    \n",
    "    print(f\"  {round(r2max, 2)} r2 to full dataset\")\n",
    "    for f0 in best_feat:\n",
    "        print(f\"    {f0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c464af",
   "metadata": {},
   "source": [
    "# Train and test ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86356a10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def train_model(\n",
    "    df: pd.DataFrame,\n",
    "    input_cols: list,\n",
    "    target_col: str,\n",
    "    splits: list,\n",
    "    model_type: str,\n",
    "    single_feat_vals: list,\n",
    "    plot_pva: bool = False,\n",
    "    scale: bool = True,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Train an ML model using a dtafrme, target column,\n",
    "    and dict of train-test splits.\n",
    "\n",
    "    Inputs:\n",
    "    df: dataframe containing the data\n",
    "    input_cols: column names which should be useed as model input feautures\n",
    "    target_col: column name to use as model output\n",
    "    model_type: type of model. Choose one of [\"NN\", \"RF\", \"linear\"]\n",
    "    splits: a list of train-test splits to use for model cross-validation.\n",
    "\n",
    "    Output:\n",
    "    results: dataframe summarizing the results of model testing\n",
    "    model: trained model\n",
    "    \"\"\"\n",
    "\n",
    "    print(\n",
    "        f\"    inputs: {len(input_cols)}, model: {model_type}, splits: {len(splits)}, samples: {len(df)}\"\n",
    "    )\n",
    "\n",
    "    if model_type == \"NN\":\n",
    "        model = MLPRegressor(random_state=1)\n",
    "    elif model_type == \"RF\":\n",
    "        # model = RandomForestRegressorCov(random_state=1)\n",
    "        model = RandomForestRegressor(random_state=0)\n",
    "    elif model_type == \"linear\":\n",
    "        model = LinearRegression()\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"model_type is not specified correctly. Try 'linear', 'NN', or 'RF'\"\n",
    "        )\n",
    "\n",
    "    starttime = time.time()\n",
    "    results = {k: [] for k in [\"split\", \"actual\", \"predicted\"]}\n",
    "    # loop over each holdout set for cross-validation\n",
    "    for si, s in enumerate(splits):\n",
    "\n",
    "        # add best feat values for this split\n",
    "        # found from feature engineering\n",
    "        df[\"single\"] = single_feat_vals[si]\n",
    "\n",
    "        # scale the input data\n",
    "        if scale:\n",
    "            scaler = RobustScaler()\n",
    "            dfs = pd.DataFrame(\n",
    "                index=df.index,\n",
    "                data=scaler.fit_transform(df.values),\n",
    "                columns=df.columns,\n",
    "            )\n",
    "\n",
    "        # get indices of training and testing rows\n",
    "        test_idx = splits[s][\"test\"]\n",
    "        train_idx = splits[s][\"train\"]\n",
    "        # get training and testing inputs and outputs\n",
    "        train_in = dfs.loc[train_idx][input_cols].values\n",
    "        train_out = dfs.loc[train_idx][target_col].values\n",
    "        test_in = dfs.loc[test_idx][input_cols].values\n",
    "        test_out = dfs.loc[test_idx][target_col].values\n",
    "\n",
    "        # fit model and make predictions on test samples\n",
    "        model.fit(train_in, train_out)\n",
    "\n",
    "        if model_type == \"RF\":\n",
    "            # for custom random forest covariance models\n",
    "            # importances = list(model.feature_importances_)\n",
    "            # pred, unc, _ = model.predict(test_in)\n",
    "            pred = model.predict(test_in)\n",
    "        elif model_type == \"NN\":\n",
    "            # for neural nets\n",
    "            # importances = [0]*len(input_cols)\n",
    "            pred = model.predict(test_in)\n",
    "        elif model_type == \"linear\":\n",
    "            # for linear models\n",
    "            # importances = model.coef_\n",
    "            pred = model.predict(test_in)\n",
    "\n",
    "        # save results\n",
    "        results[\"split\"] += [s] * len(test_out)\n",
    "        results[\"actual\"] += list(test_out)\n",
    "        results[\"predicted\"] += list(pred)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_model_inputs(input_type: str, df: pd.DataFrame, target: str) -> list:\n",
    "    \"\"\"Get the type of inputs to use for ML\"\"\"\n",
    "    if input_type == \"single\":\n",
    "        cols = [\"single\"]\n",
    "    elif input_type == \"original\":\n",
    "        cols = [c for c in df if all([\"**\" not in c, \"ln \" not in c, c != \"single\"])]\n",
    "    elif input_type == \"all\":\n",
    "        cols = list(df)\n",
    "    else:\n",
    "        raise ValueError(\"Input type is incorrect.\")\n",
    "    return [c for c in cols if c != target]\n",
    "\n",
    "\n",
    "# read datasets\n",
    "ds = utils.read_jsonzip(os.path.join(\"data\", \"datasets.json.gz\"))\n",
    "# read feature sets\n",
    "fs = utils.read_jsonzip(os.path.join(\"data\", \"features.json.gz\"))\n",
    "# read splits\n",
    "splits = utils.read_jsonzip(os.path.join(\"data\", \"splits.json.gz\"))\n",
    "split_types = [\"LOCO\", \"Random\"]\n",
    "res = []\n",
    "start_all = time.time()\n",
    "\n",
    "# loop over each dataset\n",
    "for di, dsn in enumerate(list(ds)):\n",
    "    starttime = time.time()\n",
    "    print(f\"\\n{di+1}/{len(ds)}: {dsn}\")\n",
    "\n",
    "    # get the data\n",
    "    df = pd.DataFrame(json.loads(ds[dsn][\"df\"]))\n",
    "    t = ds[dsn][\"target\"]\n",
    "\n",
    "    # loop over each model type\n",
    "    for model_type in [\"NN\", \"RF\", \"linear\"]:\n",
    "\n",
    "        # loop over each split\n",
    "        for split_type in split_types:\n",
    "            print(f\"  {split_type}\")\n",
    "            ss = splits[dsn][split_type]\n",
    "\n",
    "            # this is a list of lists of the discovered feature values for\n",
    "            # this dataset and CV strategy. each list of values corrresponds\n",
    "            # to the best feature discovered at a single CV split\n",
    "            disc_vals_list = [\n",
    "                pd.DataFrame(json.loads(fs[dsn][split_type][i])).iloc[0][\"val\"]\n",
    "                for i in range(10)\n",
    "            ]\n",
    "\n",
    "            # loop over each featurization strategy\n",
    "            for input_type in [\"single\", \"all\", \"original\"]:\n",
    "\n",
    "                # get model input columns\n",
    "                input_cols = get_model_inputs(\n",
    "                    input_type=input_type,\n",
    "                    df=df,\n",
    "                    target=t,\n",
    "                )\n",
    "\n",
    "                # print(input_cols)\n",
    "                # train and test model\n",
    "                results0 = train_model(\n",
    "                    df=df,\n",
    "                    input_cols=input_cols,\n",
    "                    target_col=t,\n",
    "                    splits={f\"split-{si}\": ss[si] for si in range(len(ss))},\n",
    "                    model_type=model_type,\n",
    "                    single_feat_vals=disc_vals_list,\n",
    "                    scale=True,\n",
    "                )\n",
    "                # append results\n",
    "                res.append(\n",
    "                    {\n",
    "                        \"dataset\": dsn,\n",
    "                        \"model\": model_type,\n",
    "                        \"split_type\": split_type,\n",
    "                        \"input_type\": input_type,\n",
    "                        \"results\": results0,\n",
    "                    }\n",
    "                )\n",
    "    print(f\"dataset runtime: {round((time.time() - starttime)/60, 2)} min\")\n",
    "\n",
    "# save datsets to file\n",
    "utils.write_jsonzip(res, os.path.join(\"data\", \"results.json.gz\"))\n",
    "\n",
    "print(f\"Total runtime: {round((time.time() - start_all)/3600, 2)} hrs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47be1c0",
   "metadata": {},
   "source": [
    "# Plot model performance across all configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafd32e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read datasets\n",
    "ds = utils.read_jsonzip(os.path.join(\"data\", \"datasets.json.gz\"))\n",
    "# read model results\n",
    "rdf = utils.get_all_model_results()\n",
    "\n",
    "\n",
    "# get all model and input types\n",
    "models = [\"linear\", \"RF\", \"NN\"]\n",
    "input_types = [\"single\", \"all\", \"original\"]\n",
    "split_types = [\"LOCO\", \"Random\"]\n",
    "bar_colors = [\"mediumorchid\", \"green\", \"darkorange\", \"gray\"] * 3\n",
    "ALPHA = 0.5\n",
    "legend_colors = {\"single\": \"mediumorchid\", \"all\": \"green\", \"original\": \"darkorange\"}\n",
    "\n",
    "fig, ax = plt.subplots(nrows=3, ncols=3)\n",
    "ax = np.ravel(ax)\n",
    "ii = 0\n",
    "HEIGHT = 0.8\n",
    "\n",
    "results_table = []\n",
    "\n",
    "# loop over each dataset\n",
    "for dsn in rdf[\"dataset\"].unique():\n",
    "\n",
    "    # df = pd.DataFrame(json.loads(ds[dsn][\"df\"]))\n",
    "    t = ds[dsn][\"target\"]\n",
    "\n",
    "    for split_type in [\"LOCO\", \"Random\"]:\n",
    "\n",
    "        ndmes = []\n",
    "        stds = []\n",
    "        results_table_row = {}\n",
    "\n",
    "        for model in models:\n",
    "            for ip in input_types:\n",
    "\n",
    "                # df00 = df0[(df0[\"model\"] == m) & (df0[\"input_type\"] == ip)]\n",
    "                # err_df0.loc[ip, m] = float(df00[\"ndme_median\"])\n",
    "                # err_df0.loc[ip, m+'_std'] = float(df00[\"ndme_std\"])\n",
    "                rdf0 = rdf[\n",
    "                    (rdf[\"dataset\"] == dsn)\n",
    "                    & (rdf[\"model\"] == model)\n",
    "                    & (rdf[\"input_type\"] == ip)\n",
    "                    & (rdf[\"split_type\"] == split_type)\n",
    "                ]\n",
    "\n",
    "                split_ndmes = []\n",
    "\n",
    "                for split in rdf0[\"split\"].unique():\n",
    "                    rdf00 = rdf0[rdf0[\"split\"] == split]\n",
    "\n",
    "                    rmse = np.sqrt(\n",
    "                        np.mean(np.square(rdf00[\"predicted\"] - rdf00[\"actual\"]))\n",
    "                    )\n",
    "                    ndme = rmse / np.std(rdf00[\"actual\"])\n",
    "\n",
    "                    split_ndmes.append(ndme)\n",
    "\n",
    "                ndme_median = np.median(split_ndmes)\n",
    "                ndmes.append(ndme_median)\n",
    "                stds.append(np.std(split_ndmes))\n",
    "                results_table_row[f\"{ip} - {model}\"] = ndme_median\n",
    "\n",
    "        if split_type == \"LOCO\":\n",
    "            results_table.append(results_table_row)\n",
    "\n",
    "        low_ndme = -np.min(ndmes) if split_type == \"LOCO\" else np.min(ndmes)\n",
    "        ax[ii].axvline(x=low_ndme, c=\"purple\", alpha=ALPHA, lw=0.5)\n",
    "\n",
    "        ax[ii].axvline(x=-1, c=\"gray\", alpha=ALPHA, lw=0.5, linestyle=\"dotted\")\n",
    "        ax[ii].axvline(x=1, c=\"gray\", alpha=ALPHA, lw=0.5, linestyle=\"dotted\")\n",
    "\n",
    "        # insert 0's in dadta so there's gaps between split_type in the plot\n",
    "        for data in [ndmes, stds]:\n",
    "            data.insert(3, 0)\n",
    "            data.insert(7, 0)\n",
    "\n",
    "        bars = ax[ii].barh(\n",
    "            np.arange(len(ndmes)),\n",
    "            width=ndmes if split_type == \"Random\" else np.negative(ndmes),\n",
    "            lw=0,\n",
    "            height=HEIGHT,\n",
    "            alpha=ALPHA,\n",
    "            xerr=stds,\n",
    "            error_kw=dict(\n",
    "                lw=0.5,\n",
    "                capsize=0,\n",
    "                capthick=0,\n",
    "                alpha=0.3,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        for bi, b in enumerate(bars):\n",
    "            b.set_color(bar_colors[bi])\n",
    "\n",
    "    ax[ii].axvline(x=0, c=\"k\", lw=0.5)\n",
    "    ax[ii].set_title(t, fontsize=FONTSIZE)\n",
    "    ax[ii].set_yticks([1, 5, 9])\n",
    "    ax[ii].set_xlim([-1.5, 1.5])\n",
    "    ax[ii].set_xticks([-1.5, -1, -0.5, 0, 0.5, 1, 1.5])\n",
    "    if ii > 5:\n",
    "        ax[ii].set_xlabel(\"NDME\")\n",
    "        # ax[ii].set_xticks([-1.5, -1, -0.5, 0, 0.5, 1, 1.5], [1.5, 1, 0.5, 0, 0.5, 1, 1.5])\n",
    "        ax[ii].set_xticklabels([\"1.5\", \"1\", \"0.5\", \"0\", \"0.5\", \"1\", \"1.5\"])\n",
    "    else:\n",
    "        ax[ii].set_xticklabels([])\n",
    "\n",
    "    if ii in [0, 3, 6]:\n",
    "        ax[ii].set_yticklabels(models)\n",
    "    else:\n",
    "        ax[ii].set_yticklabels([])\n",
    "\n",
    "    # horizontal split_type label\n",
    "    ax[ii].text(\n",
    "        0.02,\n",
    "        0.96,\n",
    "        \"Extrap.\\n(LOCO)\",\n",
    "        color=\"k\",\n",
    "        fontsize=FONTSIZE - 3,\n",
    "        ha=\"left\",\n",
    "        va=\"top\",\n",
    "        transform=ax[ii].transAxes,\n",
    "    )\n",
    "    ax[ii].text(\n",
    "        0.98,\n",
    "        0.96,\n",
    "        \"Interp.\\n(random)\",\n",
    "        color=\"k\",\n",
    "        fontsize=FONTSIZE - 3,\n",
    "        ha=\"right\",\n",
    "        va=\"top\",\n",
    "        transform=ax[ii].transAxes,\n",
    "    )\n",
    "\n",
    "    ii += 1\n",
    "\n",
    "ax[1].legend(\n",
    "    ncol=3,\n",
    "    bbox_to_anchor=(0.5, 1.25),\n",
    "    loc=\"lower center\",\n",
    "    # fontsize=16,\n",
    "    handles=[\n",
    "        mpatches.Patch(\n",
    "            color=legend_colors[k],\n",
    "            alpha=ALPHA,\n",
    "            label={\"single\": \"BE\", \"all\": \"BE + original\", \"original\": \"original\"}[k],\n",
    "            lw=0,\n",
    "        )\n",
    "        for k in legend_colors\n",
    "    ],\n",
    ")\n",
    "plt.subplots_adjust(wspace=-0.95, hspace=1)\n",
    "plt.tight_layout()\n",
    "plt.gcf().savefig(os.path.join(utils.FIG_BASEPATH, \"ModelErrorSummary.png\"))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "results_table = pd.DataFrame(results_table)\n",
    "results_table.index = [ds[dsn][\"target\"] for dsn in rdf[\"dataset\"].unique()]\n",
    "results_table = results_table.T\n",
    "results_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fecb4bc-8846-4275-93ba-6f469a989d96",
   "metadata": {},
   "source": [
    "# Extrapolation and interpolation performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00446c9d-0dde-4033-97cc-572366c04dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read datasets\n",
    "ds = utils.read_jsonzip(os.path.join(\"data\", \"datasets.json.gz\"))\n",
    "# read result sets\n",
    "rs = pd.DataFrame(utils.read_jsonzip(os.path.join(\"data\", \"results.json.gz\")))\n",
    "\n",
    "# read model results\n",
    "rdf = utils.get_all_model_results()\n",
    "\n",
    "\n",
    "models = [\"RF\", \"NN\"]\n",
    "input_types = [\"original\"]\n",
    "ALPHA = 0.5\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "ax = np.ravel(ax)\n",
    "\n",
    "colors = {\n",
    "    \"melting temp\": \"dodgerblue\",\n",
    "    \"bulk modulus\": \"tomato\",\n",
    "    \"band gap\": \"darkgreen\",\n",
    "    \"heat capacity\": \"darkorange\",\n",
    "    \"compressive strength\": \"navy\",\n",
    "    \"formation energy\": \"magenta\",\n",
    "    \"fish weight\": \"darkred\",\n",
    "    \"airfoil sound\": \"limegreen\",\n",
    "    \"abalone rings\": \"gold\",\n",
    "}\n",
    "\n",
    "# colors = {\"original\": 'blue', 'single': 'green', 'all': 'red'}\n",
    "# colors = {\"NN\": \"dodgerblue\", \"RF\": \"tomato\"}\n",
    "markers = {\"NN\": \"o\", \"RF\": \"s\"}\n",
    "\n",
    "\n",
    "for ii, split_type in enumerate([\"Random\", \"LOCO\"]):\n",
    "\n",
    "    all_x, all_y = [], []\n",
    "\n",
    "    # loop over each dataset\n",
    "    for dsn in rs[\"dataset\"].unique():\n",
    "        df = pd.DataFrame(json.loads(ds[dsn][\"df\"]))\n",
    "        t = ds[dsn][\"target\"]\n",
    "\n",
    "        # get NDME for the single-feature linear models\n",
    "        sflm_df = rdf[\n",
    "            (rdf[\"dataset\"] == dsn)\n",
    "            & (rdf[\"model\"] == \"linear\")\n",
    "            & (rdf[\"input_type\"] == \"single\")\n",
    "            & (rdf[\"split_type\"] == split_type)\n",
    "        ]\n",
    "        sflm_ndmes = []\n",
    "        for split in sflm_df[\"split\"].unique():\n",
    "            sflm_rdf0 = sflm_df[sflm_df[\"split\"] == split]\n",
    "            rmse = np.sqrt(\n",
    "                np.mean(np.square(sflm_rdf0[\"predicted\"] - sflm_rdf0[\"actual\"]))\n",
    "            )\n",
    "            ndme = rmse / np.std(sflm_rdf0[\"actual\"])\n",
    "            sflm_ndmes.append(ndme)\n",
    "\n",
    "        for model in models:\n",
    "            for ip in input_types:\n",
    "\n",
    "                rdf0 = rdf[\n",
    "                    (rdf[\"dataset\"] == dsn)\n",
    "                    & (rdf[\"model\"] == model)\n",
    "                    & (rdf[\"input_type\"] == ip)\n",
    "                    & (rdf[\"split_type\"] == split_type)\n",
    "                ]\n",
    "                compare_ndmes = []\n",
    "                for split in rdf0[\"split\"].unique():\n",
    "                    rdf00 = rdf0[rdf0[\"split\"] == split]\n",
    "                    rmse = np.sqrt(\n",
    "                        np.mean(np.square(rdf00[\"predicted\"] - rdf00[\"actual\"]))\n",
    "                    )\n",
    "                    ndme = rmse / np.std(rdf00[\"actual\"])\n",
    "                    compare_ndmes.append(ndme)\n",
    "\n",
    "                ax[ii].scatter(\n",
    "                    compare_ndmes,\n",
    "                    sflm_ndmes,\n",
    "                    label=f\"{model} - {ip}\",\n",
    "                    s=10,\n",
    "                    c=colors[t],\n",
    "                    marker=markers[model],\n",
    "                    edgecolor=\"w\",\n",
    "                    lw=0.3,\n",
    "                    alpha=ALPHA,\n",
    "                )\n",
    "\n",
    "                all_y += sflm_ndmes\n",
    "                all_x += compare_ndmes\n",
    "\n",
    "    # plot a confidence interval\n",
    "    [\n",
    "        utils.confidence_ellipse(\n",
    "            all_x,\n",
    "            all_y,\n",
    "            ax[ii],\n",
    "            n_std=contour[0],\n",
    "            facecolor=\"none\",\n",
    "            lw=0.5,\n",
    "            edgecolor=\"gray\",\n",
    "            linestyle=contour[1],\n",
    "            zorder=0,\n",
    "        )\n",
    "        for contour in [\n",
    "            (1, \"solid\"),  # (2, 'dashed'),# (3, 'dotted'),\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    medx, medy = np.median(all_x), np.median(all_y)\n",
    "    # medx, medy = np.std(all_x), np.std(all_y)\n",
    "\n",
    "    lin_frac = sum([all_y[i] > all_x[i] for i in range(len(all_x))]) / len(all_x)\n",
    "    print(split_type)\n",
    "    print(f\"Fraction in which blackbox achieved lower error: {lin_frac}\")\n",
    "\n",
    "    ax[ii].scatter(medx, medy, c=\"k\", s=100, marker=\"+\", lw=0.5)\n",
    "    print(f\"median point: {medx, medy}\")\n",
    "    print(f\"percent difference: {(medy - medx)/medx}\")\n",
    "\n",
    "    ax[ii].plot([0, 1.5], [0, 1.5], lw=0.5, color=\"gray\")\n",
    "\n",
    "    label = (\n",
    "        \"Extrapolation (LOCO CV)\"\n",
    "        if split_type == \"LOCO\"\n",
    "        else \"Interpolation (random CV)\"\n",
    "    )\n",
    "    ax[ii].set_xlim([0, 1.5])\n",
    "    ax[ii].set_ylim([0, 1.5])\n",
    "    ax[ii].set_xlabel(\"NDME (black-box model)\", fontsize=FONTSIZE)\n",
    "    ax[ii].set_ylabel(\"NDME (single-feat. lin. reg.)\", fontsize=FONTSIZE)\n",
    "    ax[ii].set_title(\n",
    "        label,\n",
    "        fontsize=FONTSIZE,\n",
    "    )\n",
    "    ax[ii].axhline(y=1, c=\"gray\", alpha=ALPHA, lw=0.5, linestyle=\"dotted\")\n",
    "    ax[ii].axvline(x=1, c=\"gray\", alpha=ALPHA, lw=0.5, linestyle=\"dotted\")\n",
    "\n",
    "\n",
    "ax[0].legend(\n",
    "    ncol=1,\n",
    "    bbox_to_anchor=(1, 0),\n",
    "    loc=\"lower right\",\n",
    "    fontsize=FONTSIZE - 6,  # facecolor='white', framealpha=1,\n",
    "    handles=[\n",
    "        mpatches.Patch(color=colors[k], alpha=ALPHA, label=k, lw=0) for k in colors\n",
    "    ],\n",
    ")\n",
    "\n",
    "fig.set_size_inches((5, 2.5))\n",
    "plt.tight_layout()\n",
    "plt.gcf().savefig(\n",
    "    os.path.join(utils.FIG_BASEPATH, \"ExtrapolationAndInterpolationNDME.png\")\n",
    ")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6 (default, Sep 26 2022, 11:37:49) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "1d19892084393325d45d5a25e70293b9e04f2eae8cee4c08fe6001436763c73f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
